# Neural Network
### Work In Progress
This is my attempt to code a neural network from scratch after studying how deep learning really works.

## Activation functions
* Linear (Identity)
* Sigmoid (Logistic)
* ReLU (Rectified linear unit)
* TanH

## Loss functions
* MSE (Mean squared error)
* Binary cross entropy

# Example: XOR
To test my module, I tried to train a network predicting the XOR gate output
![Imgur](https://i.imgur.com/VXcARZu.png)
![Imgur](https://i.imgur.com/wLL6cOD.png)
